quarkus.application.name=embedder

# Production port allocation
quarkus.http.port=39003
quarkus.http.host=0.0.0.0
quarkus.grpc.server.use-separate-server=false

# Development override
%dev.quarkus.http.port=39103
%dev.quarkus.http.host=0.0.0.0

# gRPC Client pointing to registration service via Stork/Consul
quarkus.grpc.clients.registration-service.host=registration-service
quarkus.grpc.clients.registration-service.name-resolver=stork

# gRPC Client for the embedder service (used by REST endpoints)
quarkus.grpc.clients.embedder.host=localhost
quarkus.grpc.clients.embedder.port=39103

# Stork service discovery configuration
quarkus.stork.registration-service.service-discovery.type=consul
quarkus.stork.registration-service.service-discovery.consul-host=${CONSUL_HOST:consul}
quarkus.stork.registration-service.service-discovery.consul-port=${CONSUL_PORT:8500}

# Development override - use localhost Consul in dev mode  
%dev.quarkus.stork.registration-service.service-discovery.consul-host=localhost

# Module configuration
module.name=embedder
module.type=processor
module.description=Vector embedding module for semantic processing

# Enable gRPC reflection service for production (enabled by default in dev)
quarkus.grpc.server.enable-reflection-service=true

# Health check configuration - use localhost for local development
module.health.check.host=localhost

# Enable OpenAPI and Swagger UI
quarkus.swagger-ui.always-include=true
quarkus.swagger-ui.path=/swagger-ui

# gRPC Message Size Configuration (2GB - 1 byte, max int value)
quarkus.grpc.server.max-inbound-message-size=2147483647
quarkus.grpc.clients.embedder.max-inbound-message-size=2147483647

# Logging Configuration - reduce noise
quarkus.log.level=INFO
quarkus.log.category."io.pipeline".level=DEBUG
quarkus.log.category."io.vertx.ext.consul".level=WARN
quarkus.log.category."io.quarkus.grpc".level=WARN

# JVM Memory settings for large message processing
quarkus.native.additional-build-args=-J-Xmx4g
%dev.quarkus.devservices.timeout=120s

# Test settings  
%test.quarkus.http.port=0


##TODO: leaving this in her for now, but I believe this all comes from the configuration options in our schema
# DJL configuration for ML models
djl.engine.default-engine=PyTorch
# Use GPU if available, otherwise fallback to CPU
djl.pytorch.use-gpu=true
# Number of threads for CPU inference
djl.pytorch.num-threads=4
# Memory management
djl.pytorch.memory-management=ON_SYSTEM_MEMORY_PRESSURE

##TODO: this should be working... we probably removed it at one point?
# Processing buffer configuration
processing.buffer.enabled=${PROCESSING_BUFFER_ENABLED:false}
processing.buffer.capacity=${PROCESSING_BUFFER_CAPACITY:100}
processing.buffer.directory=${PROCESSING_BUFFER_DIRECTORY:}
processing.buffer.prefix=${PROCESSING_BUFFER_PREFIX:embedder_output}

